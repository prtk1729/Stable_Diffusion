{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "zl-S0m3pkQC5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# FLUX.1 Training Notebook\n",
    "\n",
    "- Script customized by Trelis Research.\n",
    "- Based on an original script from AI Toolkit by Ostris [here](https://github.com/ostris/ai-toolkit/tree/main/notebooks).\n",
    "\n",
    "Model License:\n",
    "- FLUX Schnell is openly licensed and training works fine.\n",
    "- Perhaps FLUX Dev is a bit better quality, but it can only be used for non-commercial purposes. To use FLUX Dev, sign into HF and accept the model access here [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) - you'll then need to update the models being loaded below. [Get a READ key from huggingface](https://huggingface.co/settings/tokens/new?) and place it in the next cell after running it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3cokMT-WC6rG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 14 12:40:13 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:01:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             44W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BvAG0GKAh59G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ai-toolkit'...\n",
      "remote: Enumerating objects: 4067, done.\u001b[K\n",
      "remote: Counting objects: 100% (2485/2485), done.\u001b[K\n",
      "remote: Compressing objects: 100% (252/252), done.\u001b[K\n",
      "remote: Total 4067 (delta 2390), reused 2233 (delta 2233), pack-reused 1582 (from 4)\u001b[K\n",
      "Receiving objects: 100% (4067/4067), 29.81 MiB | 44.05 MiB/s, done.\n",
      "Resolving deltas: 100% (3080/3080), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ostris/ai-toolkit\n",
    "!mkdir -p /workspace/dataset # or !mkdir -p /content/dataset # for colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFUW4ZMmnp1V"
   },
   "source": [
    "Put your image dataset in the `/workspace/dataset` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XGZqVER_aQJW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submodule 'repositories/batch_annotator' (https://github.com/ostris/batch-annotator) registered for path 'repositories/batch_annotator'\n",
      "Submodule 'repositories/ipadapter' (https://github.com/tencent-ailab/IP-Adapter.git) registered for path 'repositories/ipadapter'\n",
      "Submodule 'repositories/leco' (https://github.com/p1atdev/LECO) registered for path 'repositories/leco'\n",
      "Submodule 'repositories/sd-scripts' (https://github.com/kohya-ss/sd-scripts.git) registered for path 'repositories/sd-scripts'\n",
      "Cloning into '/workspace/ai-toolkit/repositories/batch_annotator'...\n",
      "Cloning into '/workspace/ai-toolkit/repositories/ipadapter'...\n",
      "Cloning into '/workspace/ai-toolkit/repositories/leco'...\n",
      "Cloning into '/workspace/ai-toolkit/repositories/sd-scripts'...\n",
      "Submodule path 'repositories/batch_annotator': checked out '420e142f6ad3cc14b3ea0500affc2c6c7e7544bf'\n",
      "Submodule 'repositories/controlnet' (https://github.com/lllyasviel/ControlNet-v1-1-nightly.git) registered for path 'repositories/batch_annotator/repositories/controlnet'\n",
      "Cloning into '/workspace/ai-toolkit/repositories/batch_annotator/repositories/controlnet'...\n",
      "Submodule path 'repositories/batch_annotator/repositories/controlnet': checked out 'e2b44154b72965c5e11b1ccee941d550682e4701'\n",
      "Submodule path 'repositories/ipadapter': checked out '5a18b1f3660acaf8bee8250692d6fb3548a19b14'\n",
      "Submodule path 'repositories/leco': checked out '9294adf40218e917df4516737afb13f069a6789d'\n",
      "Submodule path 'repositories/sd-scripts': checked out 'b78c0e2a69e52ce6c79abc6c8c82d1a9cabcf05c'\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.1 requires torch==2.1.1, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd ai-toolkit && git submodule update --init --recursive && pip install -r requirements.txt -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3yZZdhFRoj2m"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# # Prompt for the token\n",
    "# hf_token = getpass.getpass('Enter your HF access token and press enter: ')\n",
    "\n",
    "# # Set the environment variable\n",
    "# os.environ['HF_TOKEN'] = hf_token\n",
    "\n",
    "# print(\"HF_TOKEN environment variable has been set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9gO2EzQ1kQC8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/workspace/ai-toolkit') # or content/ai-toolkit for colab\n",
    "from toolkit.job import run_job\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "BEWARE that if you run inference before fine-tuning, the model will already be loaded to the GPU and you probably will run out of VRAM.\n",
    "\n",
    "So, after running inference, restart the kernel, comment out this inference cell and run the notebook from the top (minus the installs, as they are already done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.33.0.dev0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from diffusers) (4.6.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.27.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.5.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (10.1.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43091c6e2c44d16a8350092efd64ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b48280511d147bfb24bf6a3c612ac3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/536 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5462bb6476be46b9b90357eec8dcc26f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 23 files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c54cdeff3cd428c898bf5257c9b293b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler/scheduler_config.json:   0%|          | 0.00/274 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cc47105d5f4eb5987234565fc01695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/special_tokens_map.json:   0%|          | 0.00/588 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b18a384a0f540de903a12d31a5c43c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer_config.json:   0%|          | 0.00/705 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2f85d3838a4c789a63db3fec2d888e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3509482146524bab9882f96a797a6415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_2/special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db90d93760b94fc4bfc51df09eeac502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_2/config.json:   0%|          | 0.00/782 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9801002c5642489c744ff94a202798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_2/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fb9aa528ef4ecf9cc0608ecbb39230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)t_encoder_2/model.safetensors.index.json:   0%|          | 0.00/19.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bec6a2a5fa4d5d9d91e2eb31b9615c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_2/tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d267b166b94bbba10b54c91eb39dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2c13ef8a664b0a9303ae10ec2891f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e006b5ac204e42299bc3870efc3676c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ce82ce30394d018b065b3ba16adacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ion_pytorch_model.safetensors.index.json:   0%|          | 0.00/121k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b78912aeaa24407afac2d0ed1ec60a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6547ea51794c90999362fde7b0de1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/config.json:   0%|          | 0.00/774 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883d75f66adc49f58592fc95f8b2369e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff39f398329471aac48e56655ef8d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f598b14891940c79e5c47593d8ac3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00001-of-00003.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4767a846c9345f5ab29d674ed1f6ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00002-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd428256fbcb475d884917e4885826c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00003-of-00003.safetensors:   0%|          | 0.00/3.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abce20effda4b2da399aa1072a77f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9df8e46a3246b6ad9e8f9f5ddef7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febb922df75c46f9a1f91b2b3912fd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a521903a844ecea02c7a1f41026a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "!pip install -U diffusers\n",
    "\n",
    "\n",
    "# from diffusers import AutoPipelineForText2Image\n",
    "# import torch\n",
    "\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "pipeline = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "# ## For running on Colab or Kaggle with a T4:\n",
    "# # This will work, but doesn't use the GPU very well and is a bit slow.\n",
    "# pipeline.enable_model_cpu_offload()  # Save VRAM by offloading the model to CPU. You can uncomment ONLY this if you want to cut VRAM but your GPU does support bfloat16\n",
    "# pipeline.enable_sequential_cpu_offload()\n",
    "# pipeline.vae.enable_slicing()\n",
    "# pipeline.vae.enable_tiling()\n",
    "# pipeline.to(torch.float16)\n",
    "\n",
    "## For running on a GPU with at least 32 GB of VRAM\n",
    "pipeline = pipeline.to(\"cuda\")\n",
    "\n",
    "# pipeline.load_lora_weights('output/ronantrelis_A40', weight_name='ronantrelis_A40.safetensors') # update the output dir and weight name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6328994b5446119ffd21811d5729cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fb865207e047f2ac290b811d08a538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = pipeline('PrateekPani A man playing a game of championship snooker, close-up').images[0]\n",
    "image.save(\"snooker_closeup_base.png\")\n",
    "\n",
    "# image = pipeline('PrateekPani A man surrounded by wind turbines and solar panels, close-up, wind-swept', height=1024, width=1824).images[0]\n",
    "# image.save(\"windturbine5.png\")\n",
    "\n",
    "# image = pipeline('PrateekPani A man taking a photograph', height=1024, width=1824).images[0]\n",
    "# image.save(\"photograph5.png\")\n",
    "\n",
    "image = pipeline('PrateekPani Professor at a chalkboard, with a large empty speech bubble', height=1024, width=1824).images[0]\n",
    "image.save(\"chalkboard5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8UUFzVRigbC"
   },
   "source": [
    "## Fine-tuning Setup\n",
    "\n",
    "This is your config. It is documented pretty well. Normally you would do this as a yaml file, but for colab, this will work. This will run as is without modification, but feel free to edit as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_t28QURYjRQO"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "job_to_run = OrderedDict([\n",
    "    ('job', 'extension'),\n",
    "    ('config', OrderedDict([\n",
    "        # this name will be the folder and filename name\n",
    "        ('name', 'PrateekPani'),\n",
    "        ('process', [\n",
    "            OrderedDict([\n",
    "                ('type', 'sd_trainer'),\n",
    "                # root folder to save training sessions/samples/weights\n",
    "                ('log_dir', 'logs'),  # log directory\n",
    "                ('log_config', OrderedDict([\n",
    "                    ('log_interval', '10')  # log interval\n",
    "                ])),\n",
    "                ('training_folder', '/workspace/output'),\n",
    "                # uncomment to see performance stats in the terminal every N steps\n",
    "                ('performance_log_every', 50),\n",
    "                ('device', 'cuda:0'),\n",
    "                # if a trigger word is specified, it will be added to captions of training data if it does not already exist\n",
    "                # alternatively, in your captions you can add [trigger] and it will be replaced with the trigger word\n",
    "                ('trigger_word', 'PrateekPani'),\n",
    "                ('network', OrderedDict([\n",
    "                    ('type', 'lora'),\n",
    "                    ('linear', 32),\n",
    "                    ('linear_alpha', 32)\n",
    "                ])),\n",
    "                ('save', OrderedDict([\n",
    "                    ('dtype', 'bfloat16'),  # precision to save\n",
    "                    ('save_every', 500),  # save every this many steps\n",
    "                    ('max_step_saves_to_keep', 3),  # how many intermittent saves to keep\n",
    "                    ('push_to_hub', True),  # how many intermittent saves to keep\n",
    "                    ('hf_repo_id', 'prtk1729/PrateekPani'), # your model slug on hf\n",
    "                    ('hf_private', True),\n",
    "                ])),\n",
    "                ('datasets', [\n",
    "                    # datasets are a folder of images. captions need to be txt files with the same name as the image\n",
    "                    # for instance image2.jpg and image2.txt. Only jpg, jpeg, and png are supported currently\n",
    "                    # images will automatically be resized and bucketed into the resolution specified\n",
    "                    OrderedDict([\n",
    "                        ('folder_path', '/workspace/dataset'),\n",
    "                        ('caption_ext', 'txt'),\n",
    "                        ('caption_dropout_rate', 0.05),  # will drop out the caption 5% of time\n",
    "                        ('shuffle_tokens', False),  # shuffle caption order, split by commas\n",
    "                        ('cache_latents_to_disk', True),  # leave this true unless you know what you're doing\n",
    "                        ('resolution', [512, 768, 1024])  # flux enjoys multiple resolutions\n",
    "                    ])\n",
    "                ]),\n",
    "                ('train', OrderedDict([\n",
    "                    ('batch_size', 1),\n",
    "                    ('steps', 1000),  # total number of steps to train 500 - 4000 is a good range\n",
    "                    ('gradient_accumulation_steps', 1),\n",
    "                    ('train_unet', True),\n",
    "                    ('train_text_encoder', False),  # probably won't work with flux\n",
    "                    ('gradient_checkpointing', True),  # need the on unless you have a ton of vram\n",
    "                    ('noise_scheduler', 'flowmatch'),  # for training only\n",
    "                    ('optimizer', 'adamw8bit'),\n",
    "                    ('lr', 1e-4),\n",
    "\n",
    "                    # uncomment this to skip the pre training sample\n",
    "                    ('skip_first_sample', True),\n",
    "\n",
    "                    # uncomment to completely disable sampling\n",
    "                    # ('disable_sampling', True),\n",
    "\n",
    "                    # uncomment to use new vell curved weighting. Experimental but may produce better results\n",
    "                    # ('linear_timesteps', True),\n",
    "\n",
    "                    # ema will smooth out learning, but could slow it down. Recommended to leave on.\n",
    "                    ('ema_config', OrderedDict([\n",
    "                        ('use_ema', True),\n",
    "                        ('ema_decay', 0.99)\n",
    "                    ])),\n",
    "\n",
    "                    # will probably need this if gpu supports it for flux, other dtypes may not work correctly\n",
    "                    ('dtype', 'bf16')\n",
    "                ])),\n",
    "                ('model', OrderedDict([\n",
    "                    # huggingface model name or path\n",
    "                    ('name_or_path', 'black-forest-labs/FLUX.1-schnell'),\n",
    "                    ('cache_dir', 'cache_dir'),\n",
    "                    ('assistant_lora_path', 'ostris/FLUX.1-schnell-training-adapter'), # Required for flux schnell training\n",
    "                    ('is_flux', True),\n",
    "                    ('quantize', True),  # run 8bit mixed precision\n",
    "                    # low_vram is painfully slow to fuse in the adapter avoid it unless absolutely necessary\n",
    "                    #('low_vram', True),  # uncomment this if the GPU is connected to your monitors. It will use less vram to quantize, but is slower.\n",
    "                ])),\n",
    "                ('sample', OrderedDict([\n",
    "                    ('sampler', 'flowmatch'),  # must match train.noise_scheduler\n",
    "                    ('sample_every', 500),  # sample every this many steps\n",
    "                    ('width', 1024),\n",
    "                    ('height', 1024),\n",
    "                    ('prompts', [\n",
    "                        # you can add [trigger] to the prompts here and it will be replaced with the trigger word\n",
    "                        # '[trigger] holding a sign that says \\'I LOVE PROMPTS!\\'',\n",
    "                        \"[trigger] A manholding a sign that says 'I LOVE PROMPTS!'\",\n",
    "                        \"[trigger] A man playing chess at the park, bomb going off in the background\",\n",
    "                        \"[trigger] A man holding a coffee cup, in a beanie, sitting at a cafe\",\n",
    "                        \"[trigger] A manis a DJ at a night club, fish eye lens, smoke machine, lazer lights, holding a martini\",\n",
    "                        \"[trigger] A man showing off his cool new t shirt at the beach, a shark is jumping out of the water in the background\",\n",
    "                    ]),\n",
    "                    ('neg', ''),  # not used on flux\n",
    "                    ('seed', 42),\n",
    "                    ('walk_seed', True),\n",
    "                    ('guidance_scale', 1), # schnell does not do guidance\n",
    "                    ('sample_steps', 4) # 1 - 4 works well\n",
    "                ]))\n",
    "            ])\n",
    "        ])\n",
    "    ])),\n",
    "    # you can add any additional meta info here. [name] is replaced with config name at top\n",
    "    ('meta', OrderedDict([\n",
    "        ('name', '[name]'),\n",
    "        ('version', '1.0')\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 14 12:53:14 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:01:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             66W /  300W |   41611MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6F1FlM2Wb3l"
   },
   "source": [
    "## Run Fine-tuning.\n",
    "\n",
    "Check your folders to the left for results. Items will be in output/LoRA/your_name_v1 In the samples folder, there are preiodic sampled. This doesnt work great with colab. They will be in /workspace/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HkajwI8gteOh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/workspace/ai-toolkit/extensions_built_in/sd_trainer/SDTrainer.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"type\": \"sd_trainer\",\n",
      "    \"log_dir\": \"logs\",\n",
      "    \"log_config\": {\n",
      "        \"log_interval\": \"10\"\n",
      "    },\n",
      "    \"training_folder\": \"/workspace/output\",\n",
      "    \"performance_log_every\": 50,\n",
      "    \"device\": \"cuda:0\",\n",
      "    \"trigger_word\": \"PrateekPani\",\n",
      "    \"network\": {\n",
      "        \"type\": \"lora\",\n",
      "        \"linear\": 32,\n",
      "        \"linear_alpha\": 32\n",
      "    },\n",
      "    \"save\": {\n",
      "        \"dtype\": \"bfloat16\",\n",
      "        \"save_every\": 500,\n",
      "        \"max_step_saves_to_keep\": 3,\n",
      "        \"push_to_hub\": true,\n",
      "        \"hf_repo_id\": \"prtk1729/PrateekPani\",\n",
      "        \"hf_private\": true\n",
      "    },\n",
      "    \"datasets\": [\n",
      "        {\n",
      "            \"folder_path\": \"/workspace/dataset\",\n",
      "            \"caption_ext\": \"txt\",\n",
      "            \"caption_dropout_rate\": 0.05,\n",
      "            \"shuffle_tokens\": false,\n",
      "            \"cache_latents_to_disk\": true,\n",
      "            \"resolution\": [\n",
      "                512,\n",
      "                768,\n",
      "                1024\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"train\": {\n",
      "        \"batch_size\": 1,\n",
      "        \"steps\": 1000,\n",
      "        \"gradient_accumulation_steps\": 1,\n",
      "        \"train_unet\": true,\n",
      "        \"train_text_encoder\": false,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"noise_scheduler\": \"flowmatch\",\n",
      "        \"optimizer\": \"adamw8bit\",\n",
      "        \"lr\": 0.0001,\n",
      "        \"skip_first_sample\": true,\n",
      "        \"ema_config\": {\n",
      "            \"use_ema\": true,\n",
      "            \"ema_decay\": 0.99\n",
      "        },\n",
      "        \"dtype\": \"bf16\"\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"name_or_path\": \"black-forest-labs/FLUX.1-schnell\",\n",
      "        \"cache_dir\": \"cache_dir\",\n",
      "        \"assistant_lora_path\": \"ostris/FLUX.1-schnell-training-adapter\",\n",
      "        \"is_flux\": true,\n",
      "        \"quantize\": true\n",
      "    },\n",
      "    \"sample\": {\n",
      "        \"sampler\": \"flowmatch\",\n",
      "        \"sample_every\": 500,\n",
      "        \"width\": 1024,\n",
      "        \"height\": 1024,\n",
      "        \"prompts\": [\n",
      "            \"[trigger] A manholding a sign that says 'I LOVE PROMPTS!'\",\n",
      "            \"[trigger] A man playing chess at the park, bomb going off in the background\",\n",
      "            \"[trigger] A man holding a coffee cup, in a beanie, sitting at a cafe\",\n",
      "            \"[trigger] A manis a DJ at a night club, fish eye lens, smoke machine, lazer lights, holding a martini\",\n",
      "            \"[trigger] A man showing off his cool new t shirt at the beach, a shark is jumping out of the water in the background\"\n",
      "        ],\n",
      "        \"neg\": \"\",\n",
      "        \"seed\": 42,\n",
      "        \"walk_seed\": true,\n",
      "        \"guidance_scale\": 1,\n",
      "        \"sample_steps\": 4\n",
      "    }\n",
      "}\n",
      "Using EMA\n",
      "\n",
      "#############################################\n",
      "# Running job: PrateekPani\n",
      "#############################################\n",
      "\n",
      "\n",
      "Running  1 process\n",
      "Loading Flux model\n",
      "Loading transformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f628f2867db4127974a5717d26c1b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing lora from the hub: ostris/FLUX.1-schnell-training-adapter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d694b59c9ec9492c8476a9eb96de3b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_lora_weights.safetensors:   0%|          | 0.00/451M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing in LoRA\n",
      "Quantizing transformer\n",
      "Loading vae\n",
      "Loading t5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5897fd306a4c26a9fffb74f8971ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac0633adc6c421b87f8c310669bfa10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing T5\n",
      "Loading clip\n",
      "making pipe\n",
      "preparing\n",
      "Loading assistant lora\n",
      "Loading assistant adapter from /root/.cache/huggingface/hub/models--ostris--FLUX.1-schnell-training-adapter/snapshots/2715e8057d640acb4519b99f1d138ed3f1ac227c/pytorch_lora_weights.safetensors\n",
      "create LoRA network. base dim (rank): 42, alpha: 42\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder: 0 modules.\n",
      "create LoRA for U-Net: 494 modules.\n",
      "enable LoRA for U-Net\n",
      "Missing keys: []\n",
      "create LoRA network. base dim (rank): 32, alpha: 32\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder: 0 modules.\n",
      "create LoRA for U-Net: 494 modules.\n",
      "enable LoRA for U-Net\n",
      "Dataset: /workspace/dataset\n",
      "  -  Preprocessing image dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 146.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -  Found 4 images\n",
      "Bucket sizes for /workspace/dataset:\n",
      "192x448: 1 files\n",
      "448x512: 1 files\n",
      "576x384: 1 files\n",
      "448x576: 1 files\n",
      "4 buckets made\n",
      "Caching latents for /workspace/dataset\n",
      " - Saving latents to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Caching latents to disk: 100%|██████████| 4/4 [00:00<00:00, 24.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: /workspace/dataset\n",
      "  -  Preprocessing image dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 31714.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -  Found 4 images\n",
      "Bucket sizes for /workspace/dataset:\n",
      "192x448: 1 files\n",
      "640x768: 1 files\n",
      "832x576: 1 files\n",
      "640x832: 1 files\n",
      "4 buckets made\n",
      "Caching latents for /workspace/dataset\n",
      " - Saving latents to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Caching latents to disk: 100%|██████████| 4/4 [00:00<00:00, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: /workspace/dataset\n",
      "  -  Preprocessing image dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 31895.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -  Found 4 images\n",
      "Bucket sizes for /workspace/dataset:\n",
      "192x448: 1 files\n",
      "896x1088: 1 files\n",
      "1216x832: 1 files\n",
      "832x1152: 1 files\n",
      "4 buckets made\n",
      "Caching latents for /workspace/dataset\n",
      " - Saving latents to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Caching latents to disk: 100%|██████████| 4/4 [00:00<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping first sample due to config setting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:   5%|▍         | 49/1000 [01:35<27:23,  1.73s/it, lr: 1.0e-04 loss: 5.345e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 2.1883s avg - train_loop, num = 10\n",
      " - 1.2875s avg - backward, num = 10\n",
      " - 0.6399s avg - predict_unet, num = 10\n",
      " - 0.2604s avg - reset_batch, num = 4\n",
      " - 0.0797s avg - calculate_loss, num = 10\n",
      " - 0.0736s avg - optimizer_step, num = 10\n",
      " - 0.0422s avg - encode_prompt, num = 10\n",
      " - 0.0025s avg - get_batch, num = 10\n",
      " - 0.0015s avg - preprocess_batch, num = 10\n",
      " - 0.0009s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0003s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  10%|▉         | 99/1000 [03:10<32:41,  2.18s/it, lr: 1.0e-04 loss: 5.481e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 1.8308s avg - train_loop, num = 10\n",
      " - 1.0456s avg - backward, num = 10\n",
      " - 0.5518s avg - predict_unet, num = 10\n",
      " - 0.2604s avg - reset_batch, num = 4\n",
      " - 0.0633s avg - optimizer_step, num = 10\n",
      " - 0.0620s avg - calculate_loss, num = 10\n",
      " - 0.0421s avg - encode_prompt, num = 10\n",
      " - 0.0021s avg - get_batch, num = 10\n",
      " - 0.0014s avg - preprocess_batch, num = 10\n",
      " - 0.0008s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0003s avg - log_to_tensorboard, num = 1\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  15%|█▍        | 149/1000 [04:45<34:58,  2.47s/it, lr: 1.0e-04 loss: 5.335e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 2.0747s avg - train_loop, num = 10\n",
      " - 1.2264s avg - backward, num = 10\n",
      " - 0.6026s avg - predict_unet, num = 10\n",
      " - 0.2574s avg - reset_batch, num = 4\n",
      " - 0.0695s avg - calculate_loss, num = 10\n",
      " - 0.0684s avg - optimizer_step, num = 10\n",
      " - 0.0418s avg - encode_prompt, num = 10\n",
      " - 0.0021s avg - get_batch, num = 10\n",
      " - 0.0013s avg - preprocess_batch, num = 10\n",
      " - 0.0008s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  20%|█▉        | 199/1000 [06:17<22:00,  1.65s/it, lr: 1.0e-04 loss: 3.753e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 1.7060s avg - train_loop, num = 10\n",
      " - 0.9700s avg - backward, num = 10\n",
      " - 0.5076s avg - predict_unet, num = 10\n",
      " - 0.2573s avg - reset_batch, num = 4\n",
      " - 0.0620s avg - optimizer_step, num = 10\n",
      " - 0.0553s avg - calculate_loss, num = 10\n",
      " - 0.0448s avg - encode_prompt, num = 10\n",
      " - 0.0021s avg - get_batch, num = 10\n",
      " - 0.0013s avg - preprocess_batch, num = 10\n",
      " - 0.0009s avg - prepare_noise, num = 10\n",
      " - 0.0002s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0003s avg - log_to_tensorboard, num = 1\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  25%|██▍       | 249/1000 [07:51<23:52,  1.91s/it, lr: 1.0e-04 loss: 4.675e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 1.8126s avg - train_loop, num = 10\n",
      " - 1.0847s avg - backward, num = 10\n",
      " - 0.5430s avg - predict_unet, num = 10\n",
      " - 0.2549s avg - reset_batch, num = 4\n",
      " - 0.0643s avg - calculate_loss, num = 10\n",
      " - 0.0640s avg - optimizer_step, num = 10\n",
      " - 0.0401s avg - encode_prompt, num = 10\n",
      " - 0.0016s avg - get_batch, num = 10\n",
      " - 0.0011s avg - preprocess_batch, num = 10\n",
      " - 0.0006s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  30%|██▉       | 299/1000 [09:28<24:06,  2.06s/it, lr: 1.0e-04 loss: 2.227e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 2.0373s avg - train_loop, num = 10\n",
      " - 1.2025s avg - backward, num = 10\n",
      " - 0.5872s avg - predict_unet, num = 10\n",
      " - 0.2568s avg - reset_batch, num = 5\n",
      " - 0.0719s avg - optimizer_step, num = 10\n",
      " - 0.0714s avg - calculate_loss, num = 10\n",
      " - 0.0415s avg - encode_prompt, num = 10\n",
      " - 0.0023s avg - get_batch, num = 10\n",
      " - 0.0012s avg - preprocess_batch, num = 10\n",
      " - 0.0007s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0003s avg - batch_cleanup, num = 10\n",
      " - 0.0005s avg - log_to_tensorboard, num = 1\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  35%|███▍      | 349/1000 [11:02<16:10,  1.49s/it, lr: 1.0e-04 loss: 4.399e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 1.8298s avg - train_loop, num = 10\n",
      " - 1.0513s avg - backward, num = 10\n",
      " - 0.5459s avg - predict_unet, num = 10\n",
      " - 0.2581s avg - reset_batch, num = 4\n",
      " - 0.0644s avg - optimizer_step, num = 10\n",
      " - 0.0595s avg - calculate_loss, num = 10\n",
      " - 0.0420s avg - encode_prompt, num = 10\n",
      " - 0.0024s avg - get_batch, num = 10\n",
      " - 0.0012s avg - preprocess_batch, num = 10\n",
      " - 0.0008s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  40%|███▉      | 399/1000 [12:35<17:43,  1.77s/it, lr: 1.0e-04 loss: 5.374e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 1.8239s avg - train_loop, num = 10\n",
      " - 1.0526s avg - backward, num = 10\n",
      " - 0.5379s avg - predict_unet, num = 10\n",
      " - 0.2570s avg - reset_batch, num = 4\n",
      " - 0.0639s avg - optimizer_step, num = 10\n",
      " - 0.0610s avg - calculate_loss, num = 10\n",
      " - 0.0419s avg - encode_prompt, num = 10\n",
      " - 0.0021s avg - get_batch, num = 10\n",
      " - 0.0012s avg - preprocess_batch, num = 10\n",
      " - 0.0008s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0003s avg - log_to_tensorboard, num = 1\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  45%|████▍     | 449/1000 [14:11<18:11,  1.98s/it, lr: 1.0e-04 loss: 1.461e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 2.1608s avg - train_loop, num = 10\n",
      " - 1.2794s avg - backward, num = 10\n",
      " - 0.6234s avg - predict_unet, num = 10\n",
      " - 0.2587s avg - reset_batch, num = 4\n",
      " - 0.0785s avg - calculate_loss, num = 10\n",
      " - 0.0744s avg - optimizer_step, num = 10\n",
      " - 0.0418s avg - encode_prompt, num = 10\n",
      " - 0.0023s avg - get_batch, num = 10\n",
      " - 0.0013s avg - preprocess_batch, num = 10\n",
      " - 0.0008s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  50%|████▉     | 499/1000 [15:46<16:54,  2.02s/it, lr: 1.0e-04 loss: 3.548e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unloading assistant lora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Images:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Generating Images:  20%|██        | 1/5 [00:04<00:19,  4.78s/it]\u001b[A\n",
      "Generating Images:  40%|████      | 2/5 [00:09<00:14,  4.75s/it]\u001b[A\n",
      "Generating Images:  60%|██████    | 3/5 [00:14<00:09,  4.74s/it]\u001b[A\n",
      "Generating Images:  80%|████████  | 4/5 [00:18<00:04,  4.73s/it]\u001b[A\n",
      "Generating Images: 100%|██████████| 5/5 [00:23<00:00,  4.72s/it]\u001b[A\n",
      "                                                                \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading assistant lora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  50%|████▉     | 499/1000 [15:46<16:54,  2.02s/it, lr: 1.0e-04 loss: 3.548e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at step 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  50%|████▉     | 499/1000 [15:59<16:54,  2.02s/it, lr: 1.0e-04 loss: 3.548e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /workspace/output/PrateekPani/optimizer.pt\n",
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 2.0870s avg - train_loop, num = 10\n",
      " - 1.2221s avg - backward, num = 10\n",
      " - 0.6124s avg - predict_unet, num = 10\n",
      " - 0.2585s avg - reset_batch, num = 4\n",
      " - 0.0728s avg - calculate_loss, num = 10\n",
      " - 0.0706s avg - optimizer_step, num = 10\n",
      " - 0.0419s avg - encode_prompt, num = 10\n",
      " - 0.0023s avg - get_batch, num = 10\n",
      " - 0.0013s avg - preprocess_batch, num = 10\n",
      " - 0.0008s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0003s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0003s avg - log_to_tensorboard, num = 1\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  55%|█████▍    | 549/1000 [17:20<15:25,  2.05s/it, lr: 1.0e-04 loss: 4.244e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 2.0100s avg - train_loop, num = 10\n",
      " - 1.1979s avg - backward, num = 10\n",
      " - 0.6160s avg - predict_unet, num = 10\n",
      " - 0.2732s avg - reset_batch, num = 4\n",
      " - 0.0693s avg - optimizer_step, num = 10\n",
      " - 0.0671s avg - calculate_loss, num = 10\n",
      " - 0.0427s avg - encode_prompt, num = 10\n",
      " - 0.0014s avg - get_batch, num = 10\n",
      " - 0.0013s avg - preprocess_batch, num = 10\n",
      " - 0.0007s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  60%|█████▉    | 599/1000 [18:54<11:35,  1.73s/it, lr: 1.0e-04 loss: 4.604e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 1.9953s avg - train_loop, num = 10\n",
      " - 1.1508s avg - backward, num = 10\n",
      " - 0.5936s avg - predict_unet, num = 10\n",
      " - 0.2784s avg - reset_batch, num = 5\n",
      " - 0.0699s avg - optimizer_step, num = 10\n",
      " - 0.0681s avg - calculate_loss, num = 10\n",
      " - 0.0419s avg - encode_prompt, num = 10\n",
      " - 0.0022s avg - get_batch, num = 10\n",
      " - 0.0013s avg - preprocess_batch, num = 10\n",
      " - 0.0008s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0004s avg - log_to_tensorboard, num = 1\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  65%|██████▍   | 649/1000 [20:30<11:05,  1.90s/it, lr: 1.0e-04 loss: 4.495e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 1.9280s avg - train_loop, num = 10\n",
      " - 1.1201s avg - backward, num = 10\n",
      " - 0.5698s avg - predict_unet, num = 10\n",
      " - 0.2702s avg - reset_batch, num = 4\n",
      " - 0.0650s avg - optimizer_step, num = 10\n",
      " - 0.0618s avg - calculate_loss, num = 10\n",
      " - 0.0426s avg - encode_prompt, num = 10\n",
      " - 0.0022s avg - get_batch, num = 10\n",
      " - 0.0013s avg - preprocess_batch, num = 10\n",
      " - 0.0008s avg - prepare_noise, num = 10\n",
      " - 0.0002s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  70%|██████▉   | 699/1000 [22:04<08:13,  1.64s/it, lr: 1.0e-04 loss: 6.318e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 1.9235s avg - train_loop, num = 10\n",
      " - 1.1200s avg - backward, num = 10\n",
      " - 0.5643s avg - predict_unet, num = 10\n",
      " - 0.2646s avg - reset_batch, num = 4\n",
      " - 0.0677s avg - optimizer_step, num = 10\n",
      " - 0.0638s avg - calculate_loss, num = 10\n",
      " - 0.0433s avg - encode_prompt, num = 10\n",
      " - 0.0023s avg - get_batch, num = 10\n",
      " - 0.0014s avg - preprocess_batch, num = 10\n",
      " - 0.0009s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0004s avg - log_to_tensorboard, num = 1\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  75%|███████▍  | 749/1000 [23:38<08:37,  2.06s/it, lr: 1.0e-04 loss: 3.640e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 1.7202s avg - train_loop, num = 10\n",
      " - 0.9806s avg - backward, num = 10\n",
      " - 0.5158s avg - predict_unet, num = 10\n",
      " - 0.2762s avg - reset_batch, num = 4\n",
      " - 0.0608s avg - optimizer_step, num = 10\n",
      " - 0.0502s avg - calculate_loss, num = 10\n",
      " - 0.0428s avg - encode_prompt, num = 10\n",
      " - 0.0023s avg - get_batch, num = 10\n",
      " - 0.0013s avg - preprocess_batch, num = 10\n",
      " - 0.0008s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  80%|███████▉  | 799/1000 [25:13<07:24,  2.21s/it, lr: 1.0e-04 loss: 6.853e-02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 1.8730s avg - train_loop, num = 10\n",
      " - 1.0867s avg - backward, num = 10\n",
      " - 0.5479s avg - predict_unet, num = 10\n",
      " - 0.2598s avg - reset_batch, num = 4\n",
      " - 0.0652s avg - optimizer_step, num = 10\n",
      " - 0.0645s avg - calculate_loss, num = 10\n",
      " - 0.0423s avg - encode_prompt, num = 10\n",
      " - 0.0020s avg - get_batch, num = 10\n",
      " - 0.0013s avg - preprocess_batch, num = 10\n",
      " - 0.0009s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0003s avg - log_to_tensorboard, num = 1\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  85%|████████▍ | 849/1000 [26:49<05:10,  2.06s/it, lr: 1.0e-04 loss: 4.762e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 1.9599s avg - train_loop, num = 10\n",
      " - 1.1666s avg - backward, num = 10\n",
      " - 0.6023s avg - predict_unet, num = 10\n",
      " - 0.2622s avg - reset_batch, num = 4\n",
      " - 0.0683s avg - optimizer_step, num = 10\n",
      " - 0.0639s avg - calculate_loss, num = 10\n",
      " - 0.0429s avg - encode_prompt, num = 10\n",
      " - 0.0014s avg - get_batch, num = 10\n",
      " - 0.0012s avg - preprocess_batch, num = 10\n",
      " - 0.0007s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  90%|████████▉ | 899/1000 [28:24<03:15,  1.93s/it, lr: 1.0e-04 loss: 5.526e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 2.2104s avg - train_loop, num = 10\n",
      " - 1.2987s avg - backward, num = 10\n",
      " - 0.6488s avg - predict_unet, num = 10\n",
      " - 0.2582s avg - reset_batch, num = 5\n",
      " - 0.0804s avg - calculate_loss, num = 10\n",
      " - 0.0731s avg - optimizer_step, num = 10\n",
      " - 0.0430s avg - encode_prompt, num = 10\n",
      " - 0.0023s avg - get_batch, num = 10\n",
      " - 0.0013s avg - preprocess_batch, num = 10\n",
      " - 0.0008s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - batch_cleanup, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0003s avg - log_to_tensorboard, num = 1\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani:  95%|█████████▍| 949/1000 [29:58<01:47,  2.11s/it, lr: 1.0e-04 loss: 4.439e-01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timer 'PrateekPani Timer':\n",
      " - 1.9070s avg - train_loop, num = 10\n",
      " - 1.0895s avg - backward, num = 10\n",
      " - 0.5747s avg - predict_unet, num = 10\n",
      " - 0.2624s avg - reset_batch, num = 4\n",
      " - 0.0681s avg - optimizer_step, num = 10\n",
      " - 0.0638s avg - calculate_loss, num = 10\n",
      " - 0.0449s avg - encode_prompt, num = 10\n",
      " - 0.0023s avg - get_batch, num = 10\n",
      " - 0.0013s avg - preprocess_batch, num = 10\n",
      " - 0.0008s avg - prepare_noise, num = 10\n",
      " - 0.0003s avg - prepare_latents, num = 10\n",
      " - 0.0002s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrateekPani: 100%|█████████▉| 999/1000 [31:29<00:01,  1.89s/it, lr: 1.0e-04 loss: 2.105e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unloading assistant lora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading assistant lora\n",
      "\n",
      "Saved to /workspace/output/PrateekPani/optimizer.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in 'interpreter_login': write_permission. Will not be supported from version '1.0'.\n",
      "\n",
      "Fine-grained tokens added complexity to the permissions, making it irrelevant to check if a token has 'write' access.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  ········\n",
      "Add token as git credential? (Y/n)  Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3da37ff77142b48e6df05a1e06bd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a3380bae8642178c383e11872ab509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PrateekPani.safetensors:   0%|          | 0.00/344M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e03fe57495f42fa9a15f3e0d7fd2d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PrateekPani_000000500.safetensors:   0%|          | 0.00/344M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_job(job_to_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hblgb5uwW5SD"
   },
   "source": [
    "Output files and samples are now in your output directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Tensorboard to view progress (optional)\n",
    "\n",
    "To do this, ssh into the terminal using VSCode/Cursor or otherwise and cd into the `workspace` directory\n",
    "\n",
    "Then run:\n",
    "```\n",
    "pip install tensorboard\n",
    "tensorboard --logdir logs/\n",
    "```\n",
    "VSCode should port the server to your localhost so you can view the tensorboard results. Note that nothing will appear until the first data has been logged (50 steps above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository 'Trelis/loraRonan' created or already exists.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b00adebb604074ad970dfb779117ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loraRonan.safetensors:   0%|          | 0.00/344M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d1d128b34b4f069cfcd55db06259b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loraRonan_000000500.safetensors:   0%|          | 0.00/344M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbfbf1ee7264f9fa940f7b6881ed3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f92b01d73949f7bccaeee857e27cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1d62406cb0470d886ecff0786e44c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loraRonan_000001000.safetensors:   0%|          | 0.00/344M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded the folder: output/loraRonan to repo: Trelis/loraRonan\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# Initialize HfApi object\n",
    "api = HfApi()\n",
    "\n",
    "# Hardcoded local folder path to upload\n",
    "local_folder_path = \"output/loraRonan\"  # Replace with your actual folder path\n",
    "\n",
    "# Hardcoded repository details\n",
    "repo_id = \"Trelis/loraRonan\"  # Replace with your actual repo ID\n",
    "repo_type = \"model\"  # Change to \"space\" or \"dataset\" if needed\n",
    "\n",
    "# Check if the repository exists, if not create it\n",
    "try:\n",
    "    # Try to create the repository (it will raise an error if it already exists)\n",
    "    create_repo(repo_id, repo_type=repo_type, exist_ok=True)\n",
    "    print(f\"Repository '{repo_id}' created or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating repository: {e}\")\n",
    "\n",
    "# Optional: ignore patterns and/or allow patterns for filtering files\n",
    "# ignore_patterns = \"**/logs/*.txt\"  # Ignore all text log files, adjust as needed\n",
    "allow_patterns = None  # You can define patterns for allowed files if necessary\n",
    "\n",
    "# Upload the folder to the repository\n",
    "try:\n",
    "    api.upload_folder(\n",
    "        folder_path=local_folder_path,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=repo_type,\n",
    "        # ignore_patterns=ignore_patterns,  # Adjust or remove based on needs\n",
    "        allow_patterns=allow_patterns     # Adjust or remove based on needs\n",
    "    )\n",
    "    print(f\"Successfully uploaded the folder: {local_folder_path} to repo: {repo_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading folder: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
